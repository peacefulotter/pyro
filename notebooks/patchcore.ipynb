{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pyroml import PyroModel, Stage, Trainer\n",
    "\n",
    "# Required to make imgaug work\n",
    "np.sctypes = dict(\n",
    "    float=[np.float16, np.float32, np.float64],\n",
    "    int=[np.int8, np.int16, np.int32, np.int64],\n",
    "    uint=[np.uint8, np.uint16, np.uint32, np.uint64],\n",
    ")\n",
    "\n",
    "\n",
    "from anomalib.models.components import DynamicBufferMixin, KCenterGreedy  # noqa: E402\n",
    "from anomalib.models.image.patchcore.anomaly_map import (  # noqa: E402\n",
    "    AnomalyMapGenerator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchcoreModel(DynamicBufferMixin, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        num_neighbors: int = 9,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.feature_pooler = nn.AvgPool2d(3, 1, 1)\n",
    "        self.feature_extractor = backbone\n",
    "        self.anomaly_map_generator = AnomalyMapGenerator()\n",
    "        self.register_buffer(\"memory_bank\", torch.Tensor())\n",
    "        self.memory_bank: torch.Tensor\n",
    "\n",
    "    def postprocess_features(\n",
    "        self, features: dict[str, torch.Tensor]\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        pp_features = {}\n",
    "\n",
    "        for layer, feature in features.items():\n",
    "            feature = self.feature_pooler(feature)\n",
    "\n",
    "            pp_features[layer] = feature\n",
    "\n",
    "        return pp_features\n",
    "\n",
    "    def get_embedding(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        self.feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(input_tensor)\n",
    "        features = self.postprocess_features(features)\n",
    "        embedding = self.generate_embedding(features)\n",
    "        return embedding\n",
    "\n",
    "    def embed(self, input_tensor: torch.Tensor) -> tuple[torch.Tensor, int, int, int]:\n",
    "        embedding = self.get_embedding(input_tensor)\n",
    "        B, _, W, H = embedding.shape\n",
    "        embedding = self.reshape_embedding(embedding)\n",
    "        return embedding, B, W, H\n",
    "\n",
    "    def embedding_to_score(\n",
    "        self, embedding: torch.Tensor, B: int, W: int, H: int\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        # apply nearest neighbor search\n",
    "        patch_scores, locations = self.nearest_neighbors(\n",
    "            embedding=embedding, n_neighbors=1\n",
    "        )\n",
    "        # reshape to batch dimension\n",
    "        patch_scores = patch_scores.reshape((B, -1))\n",
    "        locations = locations.reshape((B, -1))\n",
    "        # compute anomaly score\n",
    "        pred_score = self.compute_anomaly_score(patch_scores, locations, embedding)\n",
    "        # reshape to w, h\n",
    "        patch_scores = patch_scores.reshape((B, 1, W, H))\n",
    "        # get anomaly map\n",
    "        patch_scores = patch_scores.float()  # in case use_bfloat16 is True\n",
    "        anomaly_map = patch_scores\n",
    "        output = {\"anomaly_map\": anomaly_map, \"pred_score\": pred_score}\n",
    "        return output\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor):\n",
    "        \"\"\"Return Embedding during training, or a tuple of anomaly map and anomaly score during testing.\n",
    "\n",
    "        Steps performed:\n",
    "        1. Get features from a CNN.\n",
    "        2. Generate embedding based on the features.\n",
    "        3. Compute anomaly map in test mode.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor | dict[str, torch.Tensor]: Embedding for training, anomaly map and anomaly score for testing.\n",
    "        \"\"\"\n",
    "\n",
    "        embedding, B, W, H = self.embed(input_tensor)\n",
    "\n",
    "        if self.training:\n",
    "            output = embedding\n",
    "\n",
    "        elif len(self.memory_bank) == 0:\n",
    "            warnings.warn(\"empty memory bank during eval / test\")\n",
    "            output = {\n",
    "                \"anomaly_map\": torch.zeros_like(input_tensor),\n",
    "                \"pred_score\": torch.zeros((B)),\n",
    "            }\n",
    "        else:\n",
    "            output = self.embedding_to_score(embedding, B, W, H)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate_embedding(self, features) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding from hierarchical feature map.\n",
    "\n",
    "        Args:\n",
    "            features: Hierarchical feature map from a CNN (ResNet18 or WideResnet)\n",
    "            features: dict[str:Tensor]:\n",
    "\n",
    "        Returns:\n",
    "            Embedding vector\n",
    "        \"\"\"\n",
    "\n",
    "        layers = list(features.keys())\n",
    "        embeddings = features[layers[0]]\n",
    "\n",
    "        for layer in layers[1:]:\n",
    "            layer_embedding = features[layer]\n",
    "            layer_embedding = F.interpolate(\n",
    "                layer_embedding, size=embeddings.shape[-2:], mode=\"bilinear\"\n",
    "            )\n",
    "            embeddings = torch.cat((embeddings, layer_embedding), 1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape_embedding(embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Reshape Embedding.\n",
    "\n",
    "        Reshapes Embedding to the following format:\n",
    "            - [Batch, Embedding, Patch, Patch] to [Batch*Patch*Patch, Embedding]\n",
    "\n",
    "        Args:\n",
    "            embedding (torch.Tensor): Embedding tensor extracted from CNN features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Reshaped embedding tensor.\n",
    "        \"\"\"\n",
    "        embedding_size = embedding.size(1)\n",
    "        return embedding.permute(0, 2, 3, 1).reshape(-1, embedding_size)\n",
    "\n",
    "    def subsample_embedding(\n",
    "        self, embedding: torch.Tensor, sampling_ratio: float\n",
    "    ) -> None:\n",
    "        \"\"\"Subsample embedding based on coreset sampling and store to memory.\n",
    "\n",
    "        Args:\n",
    "            embedding (np.ndarray): Embedding tensor from the CNN\n",
    "            sampling_ratio (float): Coreset sampling ratio\n",
    "        \"\"\"\n",
    "\n",
    "        # Coreset Subsampling\n",
    "        sampler = KCenterGreedy(embedding=embedding, sampling_ratio=sampling_ratio)\n",
    "        coreset = sampler.sample_coreset()\n",
    "        self.memory_bank = coreset\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean_dist(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Calculate pair-wise distance between row vectors in x and those in y.\n",
    "\n",
    "        Replaces torch cdist with p=2, as cdist is not properly exported to onnx and openvino format.\n",
    "        Resulting matrix is indexed by x vectors in rows and y vectors in columns.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor 1\n",
    "            y: input tensor 2\n",
    "\n",
    "        Returns:\n",
    "            Matrix of distances between row vectors in x and y.\n",
    "\n",
    "        \"\"\"\n",
    "        x_norm = x.pow(2).sum(dim=-1, keepdim=True)  # |x|\n",
    "        y_norm = y.pow(2).sum(dim=-1, keepdim=True)  # |y|\n",
    "\n",
    "        # row distance can be rewritten as sqrt(|x| - 2 * x @ y.T + |y|.T)\n",
    "        res = (\n",
    "            x_norm - 2 * torch.matmul(x, y.transpose(-2, -1)) + y_norm.transpose(-2, -1)\n",
    "        )\n",
    "        return res.clamp_min_(0).sqrt_()\n",
    "\n",
    "    def nearest_neighbors(\n",
    "        self, embedding: torch.Tensor, n_neighbors: int\n",
    "    ):  # -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Nearest Neighbours using brute force method and euclidean norm.\n",
    "\n",
    "        Args:\n",
    "            embedding (torch.Tensor): Features to compare the distance with the memory bank.\n",
    "            n_neighbors (int): Number of neighbors to look at\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Patch scores.\n",
    "            Tensor: Locations of the nearest neighbor(s).\n",
    "        \"\"\"\n",
    "        distances = self.euclidean_dist(embedding, self.memory_bank)\n",
    "\n",
    "        if n_neighbors == 1:\n",
    "            # when n_neighbors is 1, speed up computation by using min instead of topk\n",
    "            patch_scores, locations = distances.min(1)\n",
    "        else:\n",
    "            patch_scores, locations = distances.topk(\n",
    "                k=n_neighbors, largest=False, dim=1\n",
    "            )\n",
    "\n",
    "        return patch_scores, locations\n",
    "\n",
    "    def compute_anomaly_score(\n",
    "        self,\n",
    "        patch_scores: torch.Tensor,\n",
    "        locations: torch.Tensor,\n",
    "        embedding: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute Image-Level Anomaly Score.\n",
    "\n",
    "        Args:\n",
    "            patch_scores (torch.Tensor): Patch-level anomaly scores\n",
    "            locations: Memory bank locations of the nearest neighbor for each patch location\n",
    "            embedding: The feature embeddings that generated the patch scores\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Image-level anomaly scores\n",
    "        \"\"\"\n",
    "        # Don't need to compute weights if num_neighbors is 1\n",
    "        if self.num_neighbors == 1:\n",
    "            return patch_scores.amax(1)\n",
    "\n",
    "        batch_size, num_patches = patch_scores.shape\n",
    "\n",
    "        # 1. Find the patch with the largest distance to it's nearest neighbor in each image\n",
    "        # indices of m^test,* in the paper\n",
    "        max_patches = torch.argmax(patch_scores, dim=1)\n",
    "\n",
    "        # m^test,* in the paper\n",
    "        max_patches_features = embedding.reshape(batch_size, num_patches, -1)[\n",
    "            torch.arange(batch_size), max_patches\n",
    "        ]\n",
    "\n",
    "        # 2. Find the distance of the patch to it's nearest neighbor, and the location of the nn in the membank\n",
    "        score = patch_scores[torch.arange(batch_size), max_patches]  # s^* in the paper\n",
    "\n",
    "        if self.num_neighbors > 0:\n",
    "            # indices of m^* in the paper\n",
    "            nn_index = locations[torch.arange(batch_size), max_patches]\n",
    "\n",
    "            # 3. Find the support samples of the nearest neighbor in the membank\n",
    "            nn_sample = self.memory_bank[nn_index, :]  # m^* in the paper\n",
    "\n",
    "            # indices of N_b(m^*) in the paper\n",
    "            # edge case when memory bank is too small\n",
    "            memory_bank_effective_size = self.memory_bank.shape[0]\n",
    "\n",
    "            _, support_samples = self.nearest_neighbors(\n",
    "                nn_sample,\n",
    "                n_neighbors=min(self.num_neighbors, memory_bank_effective_size),\n",
    "            )\n",
    "\n",
    "            # 4. Find the distance of the patch features to each of the support samples\n",
    "            distances = self.euclidean_dist(\n",
    "                max_patches_features.unsqueeze(1), self.memory_bank[support_samples]\n",
    "            )\n",
    "\n",
    "            # 5. Apply softmax to find the weights\n",
    "            weights = (1 - F.softmax(distances.squeeze(1), 1))[..., 0]\n",
    "\n",
    "        else:\n",
    "            weights = 1\n",
    "\n",
    "        # 6. Apply the weight factor to the score\n",
    "        return weights * score  # s in the paper\n",
    "\n",
    "\n",
    "class PyroPatchcore(PyroModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        coreset_sampling_ratio: float = 0.1,\n",
    "        num_neighbors: int = 9,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = PatchcoreModel(\n",
    "            backbone=backbone,\n",
    "            num_neighbors=num_neighbors,\n",
    "        )\n",
    "        self.coreset_sampling_ratio = coreset_sampling_ratio\n",
    "        self.embeddings: list[torch.Tensor] = []\n",
    "\n",
    "    def training_step(self, batch) -> None:\n",
    "        embedding = self.model(batch[\"img\"])\n",
    "        self.embeddings.append(embedding.cpu())\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        # Get anomaly maps and predicted scores from the model.\n",
    "        output = self.model(batch[\"img\"])\n",
    "        # Add anomaly maps and predicted scores to the batch.\n",
    "        del batch[\"img\"]\n",
    "        batch[\"anomaly_maps\"] = output[\"anomaly_map\"].cpu()\n",
    "        batch[\"pred_scores\"] = output[\"pred_score\"].cpu()\n",
    "        return batch\n",
    "\n",
    "    def step(self, batch: dict[str, torch.Tensor], stage: Stage):\n",
    "        if stage == Stage.TRAIN:\n",
    "            return self.training_step(batch)\n",
    "        return self.validation_step(batch)\n",
    "\n",
    "    def configure_optimizers(self, trainer: Trainer):\n",
    "        pass\n",
    "\n",
    "    def _fit(self, loss: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, args):\n",
    "        self.construct()\n",
    "\n",
    "    def anomaly_map(self, patch_scores, image_size=256):\n",
    "        return self.model.anomaly_map_generator(patch_scores, [image_size, image_size])\n",
    "\n",
    "    def construct(self) -> None:\n",
    "        print(\"Aggregating the embedding extracted from the training set.\")\n",
    "        self.cpu()\n",
    "        embeddings = torch.vstack(self.embeddings).to(self.trainer.device)\n",
    "        print(\"Applying core-set subsampling to get the embedding.\")\n",
    "        self.model.subsample_embedding(embeddings, self.coreset_sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone has 2,782,784 params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'layer2': torch.Size([1, 128, 28, 28]),\n",
       " 'layer3': torch.Size([1, 256, 14, 14])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyroml.models.backbone import Backbone\n",
    "\n",
    "\n",
    "# backbone = Backbone.load(\n",
    "#     \"vit_base_patch16_224.mae\", pre_trained=True, image_size=(3, 224, 224)\n",
    "# )\n",
    "backbone = Backbone.load(\n",
    "    \"resnet18\", pre_trained=True, layers=(2, 3), image_size=(3, 224, 224)\n",
    ")\n",
    "backbone.feature_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_size': (3, 224, 224),\n",
       "  'interpolation': 'bicubic',\n",
       "  'mean': (0.485, 0.456, 0.406),\n",
       "  'std': (0.229, 0.224, 0.225),\n",
       "  'crop_pct': 0.875,\n",
       "  'crop_mode': 'center'},\n",
       " Compose(\n",
       "     Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     MaybeToTensor()\n",
       "     Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       " ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.data import resolve_model_data_config, create_transform\n",
    "\n",
    "data_config = resolve_model_data_config(backbone)\n",
    "transforms = create_transform(**data_config, is_training=False)\n",
    "data_config, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The requested operation requires that 'huggingface_hub>=0.20.0' is installed on your machine, but found 'huggingface_hub==0.19.4'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file fiftyone.yml from Voxel51/mvtec-ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fiftyone.yml: 100%|██████████| 127/127 [00:00<?, ?B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m fo\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrequirement_error_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Note: other available arguments include 'max_samples', etc\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfouh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoxel51/mvtec-ad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fiftyone\\utils\\huggingface.py:323\u001b[0m, in \u001b[0;36mload_from_hub\u001b[1;34m(repo_id, revision, split, splits, subset, subsets, max_samples, batch_size, num_workers, overwrite, persistent, name, token, config_file, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find fiftyone metadata for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_dataset_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fiftyone\\utils\\huggingface.py:899\u001b[0m, in \u001b[0;36m_load_dataset_from_config\u001b[1;34m(config, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_parquet_files_dataset_from_config(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_fiftyone_dataset_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fiftyone\\utils\\huggingface.py:1473\u001b[0m, in \u001b[0;36m_load_fiftyone_dataset_from_config\u001b[1;34m(config, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_type_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiftyOneDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1471\u001b[0m     \u001b[38;5;66;03m# If the dataset is a FiftyOneDataset, download only the necessary files\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_progress_bars():\n\u001b[1;32m-> 1473\u001b[0m         \u001b[43mhfh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1478\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_progress_bars():\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\_snapshot_download.py:187\u001b[0m, in \u001b[0;36msnapshot_download\u001b[1;34m(repo_id, repo_type, revision, cache_dir, local_dir, local_dir_use_symlinks, library_name, library_version, user_agent, proxies, etag_timeout, resume_download, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, endpoint)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# if we have internet connection we retrieve the correct folder name from the huggingface api\u001b[39;00m\n\u001b[0;32m    186\u001b[0m api \u001b[38;5;241m=\u001b[39m HfApi(library_name\u001b[38;5;241m=\u001b[39mlibrary_name, library_version\u001b[38;5;241m=\u001b[39mlibrary_version, user_agent\u001b[38;5;241m=\u001b[39muser_agent, endpoint\u001b[38;5;241m=\u001b[39mendpoint)\n\u001b[1;32m--> 187\u001b[0m repo_info \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m repo_info\u001b[38;5;241m.\u001b[39msha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo info returned from server must have a revision sha.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m repo_info\u001b[38;5;241m.\u001b[39msiblings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo info returned from server must have a siblings list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2112\u001b[0m, in \u001b[0;36mHfApi.repo_info\u001b[1;34m(self, repo_id, revision, repo_type, timeout, files_metadata, token)\u001b[0m\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported repo type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1987\u001b[0m, in \u001b[0;36mHfApi.dataset_info\u001b[1;34m(self, repo_id, revision, timeout, files_metadata, token)\u001b[0m\n\u001b[0;32m   1985\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1986\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m-> 1987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDatasetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\echo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hf_api.py:626\u001b[0m, in \u001b[0;36mDatasetInfo.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikes \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlikes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaperswithcode_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaperswithcode_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags \u001b[38;5;241m=\u001b[39m \u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m card_data \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardData\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcard_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcard_data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    629\u001b[0m     DatasetCardData(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcard_data, ignore_metadata_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(card_data, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m card_data\n\u001b[0;32m    630\u001b[0m )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tags'"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "fo.config.requirement_error_level = 1\n",
    "# Load the dataset\n",
    "# Note: other available arguments include 'max_samples', etc\n",
    "dataset = fouh.load_from_hub(\"Voxel51/mvtec-ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582351f1b1e141e982e24425a660ce04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25c1c8792d4401994d7c3c81b5f5984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m tr_ds \u001b[38;5;241m=\u001b[39m MVTecDataset(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m      6\u001b[0m tr_ds, te_ds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(tr_ds, [\u001b[38;5;241m2500\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tr_ds) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2500\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mlen\u001b[39m(tr_ds), \u001b[38;5;28mlen\u001b[39m(te_ds), \u001b[43mtr_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[1;32mc:\\dev\\py\\pyroml\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mC:\\dev\\py\\pyroml\\pyroml\\template\\mvtec\\dataset.py:42\u001b[0m, in \u001b[0;36mMVTecDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(item\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 42\u001b[0m     item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "\u001b[1;31mKeyError\u001b[0m: 'img'"
     ]
    }
   ],
   "source": [
    "from pyroml.template.mvtec.dataset import MVTecDataset\n",
    "from timm.data.transforms_factory import transforms_imagenet_eval\n",
    "\n",
    "transform = transforms_imagenet_eval()\n",
    "tr_ds = MVTecDataset(split=\"train\", transform=transform)\n",
    "tr_ds, te_ds = torch.utils.data.random_split(tr_ds, [2500, len(tr_ds) - 2500])\n",
    "\n",
    "len(tr_ds), len(te_ds), tr_ds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.model.forward_features(torch.rand(1, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyroPatchcore(backbone, coreset_sampling_ratio=0.01, num_neighbors=9)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyroml.callbacks.progress.tqdm_progress import TQDMProgress\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    lr=0,\n",
    "    max_epochs=1,\n",
    "    evaluate_on=False,\n",
    "    device=\"cuda\",\n",
    "    pin_memory=False,\n",
    "    dtype=torch.float16,\n",
    "    wandb=False,\n",
    "    callbacks=[TQDMProgress(stack_bars=False)],\n",
    ")\n",
    "\n",
    "trainer.fit(model, tr_dataset=tr_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
